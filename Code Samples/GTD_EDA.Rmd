---
title: 'Exploring the Global Terrosism Data: The Backend'
author: "Suhruth Yambakam"
date: "7/19/2021"
output: html_document
---

&emsp;The Global Terrorism Database or simply called GTD is an unclassified database for terrorist attacks in the world.
 
&emsp;In this post we are going to see how I performed the correspondance analysis and Topic modelling to a subset of GTD dataset to uncover hidden patterns.

> Note: The datset used to perform these tasks are subsetted using SQLite3 queries in Database Browser.I hope you can reproduce them.

```{r  message=FALSE}
# Loading the necessary  libraries
library(dplyr) # data manupulation library
library(reshape2) # used to reshaping the data
library(ca) # to perform correspondance analysis 
library(stringr) # string manupulation library
library(tidyverse) # collection of tidymodels packages
library(tidytext) # used for text mining
library(ggplot2) # used for data visualization
```

## Simple Correspondance Analysis
&emsp;Firstly I wrote a utitlit function to perform ca and obtain the required values to plot a graph.The function goes as follows.
```{r}
names <- c("1-2", "3-9", "10-29" ,">=30") # creating a vector of fatality levels

# creating a fucntion to  perform CA
simple_ca <- function(df, formula) {
  
  regCT <- dcast(df, formula) # dcasting the data by provided formula
  
  regCT[is.na(regCT)] <- 0 # assigning null a zero value
  
  fit <- ca(regCT[, names]) # performing the CA
  
  tmp <- data.frame(name=regCT$txt,chsqdist=fit$rowdist, fit$rowcoord,
                    inertia=fit$rowinertia) # creating a dataframe of necessary  values
  
  tmp2 <- data.frame(name=c("Very High" ,"Very Low", "High", "Low"),
                     chsqdist=fit$coldist, fit$colcoord, inertia=fit$colinertia) # getting the CA values
  
  
  tmp3 <- rbind(tmp, tmp2) # row binding the dataframes
  
  plot(fit) # plotting the cA
  
  return(tmp3) # returning the required data
}
```

&emsp;Now let's load the dataframe for regions
```{r}
regDf <- read.csv("../../data/sub_data/Fatality Freq by Region.csv") # loading the region data
names(regDf) <- c("txt", "Region", "FatalityLevel", "Count") # assigning custom colnames
 
head(regDf) # printing the 1st few rows
```

&emsp;performing the analysis and writing the files into a csv
```{r}
rfDf <- simple_ca(regDf, Region+txt~FatalityLevel) # performing CA
head(rfDf) # printing the 1st few rows


write.csv(rfDf, "../../data/sub_data/CA.xlsx") #saving it to a csv
```

&emsp;Similarly with other dataframes
```{r}
# CA on  country data
cntryDf <- read.csv("../../data/sub_data/Fatality Freq by Country.csv") #loading the country data
names(cntryDf) <- c("txt", "Country", "FatalityLevel", "Count") # naming the columns

head(cntryDf) # prinitng the 1st few rows

cfDf <- simple_ca(cntryDf, Country+txt~FatalityLevel) # performing CA
head(cfDf) # printing the 1st few rows

# CA on weapons data 
weapDf <- read.csv("../../data/sub_data/Fatality Freq by Weapon Type1.csv") #loading the weapons data
names(weapDf) <- c("txt", "Weapon", "FatalityLevel", "Count") # naming the columns

head(weapDf) # printing the 1st few rows

wfDf <- simple_ca(weapDf, Weapon+txt~FatalityLevel) # performing cA
head(wfDf) # prinitng the 1st few rows

# CA on target data
targDf <- read.csv("../../data/sub_data/Fatality Freq by Target Type1.csv") # loading the target data
names(targDf) <- c("txt", "Target", "FatalityLevel", "Count") # naming the columns

head(targDf) # printing the 1st few rows

tfDf <- simple_ca(targDf, Target+txt~FatalityLevel) # performing CA
head(tfDf) # printing the 1st  few rows 

# CA on attack data
atckDf <- read.csv("../../data/sub_data/Fatality Freq by Attack Type1.csv") # loading the attack data
names(atckDf) <- c("txt", "Attack", "FatalityLevel", "Count") # naming columns

head(atckDf) # printing 1st few rows

afDf <- simple_ca(atckDf, Attack+txt~FatalityLevel) # performing CA
head(afDf) # printing the 1st few rows
```

## Topic Modelling
&emsp;I have choosen the data of attack summary, weapon details and motive of attack only for past 2 decades.
```{r}
rawDf <- tibble(read.csv("../../data/sub_data/topics.csv")) # loading the topics modeling subset

rawDf$iyear <- as.integer(as.character(rawDf$iyear)) # converting year to integer

rawDf$iyear <- factor(rawDf$iyear) # changing year to factor

rawDf <- rawDf %>% mutate_if(is.character, list(~na_if(.,""))) # converting blanks to NULL

rawDf # printing 1st few rows
```


&emsp;Performing topic modeling on attack Summary and extracting required data
```{r}
## Topic modelling on Summary
SumDf <- rawDf %>% select(iyear, summary) %>% drop_na() # creating a subset of raw data

# cleaning the string 
SumDf$summary %<>%  
  str_replace_all("[^A-z]", " ") %>% # excluding numericals
  str_replace_all("[\\s]+", " ") %>% # excluding whitespace
  tolower() # converting to lowercase

# tokenization
SumDf %<>% 
  mutate(line = row_number()) %>% # generating wor numbers
  unnest_tokens(word, summary) %>% # unnesting into words
  anti_join(stop_words) %>%  # removing stop words
  filter(word != "responsibility" & word!="claimed") # removing outliers

# calculating frequency
Sum_tfIdf <- SumDf %>% 
  count(iyear, word, sort = TRUE) %>% # count each word by year 
  bind_tf_idf(word, iyear, n) %>%  
  arrange(-tf_idf) %>% # arrange in the order of tf-idf
  group_by(iyear) %>% distinct(n, .keep_all = TRUE) %>% # extract distinct count
  top_n(10) %>% ungroup() # filter top 10

# plotting the tf-idf
g1 <- Sum_tfIdf %>%
  mutate(word=reorder_within(word, tf_idf, iyear))%>% # reorder words
  ggplot(aes(word, tf_idf, fill=iyear)) + # initializing the gggplot object
  geom_col(alpha=0.8, show.legend = F) + # creating a column/bar chart
  facet_wrap(~iyear, scales = "free", ncol = 5) + # wraping the year with free scaling 
  scale_x_reordered() + # reordering scales
  coord_flip() # flipping the coordinates
g1 

gd1 <- ggplot_build(g1)$data # getting the plot data

glimpse(gd1) # printing the plot data

write.csv(gd1, "../../data/sub_data/topic modellling/topic.csv") # saving the plot data
```

&emsp;similarly you can do for other data frames
```{r}
## Topic Modeling Terrorists Motive
motDf <- rawDf %>% select(iyear, motive) %>% drop_na() # Creating a subset

motDf <- motDf[(motDf$motive!="Unknown"),] # removing unknown values

# text  cleaning 
motDf$motive %<>%  
  str_replace_all("[^A-z]", " ") %>% # extract only aphabets
  str_replace_all("[\\s]+", " ") %>% # removing white spaces
  tolower() # converting to lowercase

# tokenization
motDf %<>% 
  mutate(line = row_number()) %>% # creating s row number
  unnest_tokens(word, motive) %>% # unnesting words
  anti_join(stop_words) # removing stop words

# calculating tf-idf 
mot_tfIdf <- motDf %>% 
  count(iyear, word, sort = TRUE) %>% # counting words by year
  bind_tf_idf(word, iyear, n) %>% 
  arrange(-tf_idf) %>% # arrange by tf-idf value
  group_by(iyear) %>% distinct(n, .keep_all = TRUE) %>% # extract distinct count
  top_n(10) %>% ungroup() # filter top 10

# plotting the motives
g2 <- mot_tfIdf %>%
  mutate(word=reorder_within(word, tf_idf, iyear))%>% # order by tf-idf
  ggplot(aes(word, tf_idf, fill=iyear)) + # initializing the ggplot
  geom_col(alpha=0.8, show.legend = F) + # creating colum chart
  facet_wrap(~iyear, scales = "free", ncol = 5) + # adding facet by year with free scale 
  scale_x_reordered() + # re-order scaling 
  coord_flip() # filp the coordinates 
g2 

gd2 <- ggplot_build(g2)$data # extract the plot data

glimpse(gd2) # print the data

## Topics modeling Weapons detail
weapDf <- rawDf %>% select(iyear, weapdetail) %>% drop_na() # creating a subset

# text cleaning 
weapDf$weapdetail %<>%  
  str_replace_all("[^A-z]", " ") %>% # filter alphabets
  str_replace_all("[\\s]+", " ") %>% # removing white space
  tolower() # converting to lower

# tokenization
weapDf %<>% 
  mutate(line = row_number()) %>% # creating a row number
  unnest_tokens(word, weapdetail) %>% # unnesting into words
  anti_join(stop_words) # removing stopwords

# creating the frequency
weap_tfIdf <- weapDf %>% 
  count(iyear, word, sort = TRUE) %>% # groupby year and count words
  bind_tf_idf(word, iyear, n) %>% 
  arrange(-tf_idf) %>% # order by tf-idf
  group_by(iyear) %>% distinct(n, .keep_all = TRUE) %>% # extract  distinct count
  top_n(10) %>% ungroup() # filter top 10

# plotting the tf-idf
g3 <- weap_tfIdf %>%
  mutate(word=reorder_within(word, tf_idf, iyear))%>% # reorder by year and tf-tdf
  ggplot(aes(word, tf_idf, fill=iyear)) + # initialize ggplot
  geom_col(alpha=0.8, show.legend = F) + # create a column chart
  facet_wrap(~iyear, scales = "free", ncol = 5) + # wraping year with free scaling
  scale_x_reordered() + # reorder scales 
  coord_flip() # flipping the coordinates
g3

gd3 <- ggplot_build(g3)$data # extract the plot data

glimpse(gd3) # printing the data

```

&emsp;After all of these I have combined them using Tableau joins to obtain my final dataframe.


